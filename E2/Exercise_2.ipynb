{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Tokenize the sentences in fileyelp_polarity.txt, removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>[wow, loved, this, place]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>[crust, is, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, tasty, and, the, texture, was, just, nasty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stopped, by, during, the, late, may, bank, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, selection, on, the, menu, was, great, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, think, food, should, have, flavor, and, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>[appetite, instantly, gone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[overall, i, was, not, impressed, and, would, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, whole, experience, was, underwhelming, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[then, as, if, i, hadnt, wasted, enough, of, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1  \\\n",
       "0                             Wow... Loved this place.  1   \n",
       "1                                   Crust is not good.  0   \n",
       "2            Not tasty and the texture was just nasty.  0   \n",
       "3    Stopped by during the late May bank holiday of...  1   \n",
       "4    The selection on the menu was great and so wer...  1   \n",
       "..                                                 ... ..   \n",
       "995  I think food should have flavor and texture an...  0   \n",
       "996                           Appetite instantly gone.  0   \n",
       "997  Overall I was not impressed and would not go b...  0   \n",
       "998  The whole experience was underwhelming, and I ...  0   \n",
       "999  Then, as if I hadn't wasted enough of my life ...  0   \n",
       "\n",
       "                                                tokens  \n",
       "0                            [wow, loved, this, place]  \n",
       "1                               [crust, is, not, good]  \n",
       "2    [not, tasty, and, the, texture, was, just, nasty]  \n",
       "3    [stopped, by, during, the, late, may, bank, ho...  \n",
       "4    [the, selection, on, the, menu, was, great, an...  \n",
       "..                                                 ...  \n",
       "995  [i, think, food, should, have, flavor, and, te...  \n",
       "996                        [appetite, instantly, gone]  \n",
       "997  [overall, i, was, not, impressed, and, would, ...  \n",
       "998  [the, whole, experience, was, underwhelming, a...  \n",
       "999  [then, as, if, i, hadnt, wasted, enough, of, m...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "import collections, functools, operator \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def read_file(filename):\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", header=None)\n",
    "    df = df.assign(tokens =df[0].apply(lambda x : x.translate(str.maketrans('', '', string.punctuation)).lower().split()))\n",
    "    return df\n",
    "\n",
    "df = read_file(\"yelp_polarity.txt\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is read using pandas and saved in the dataframe df. A function is then applied to all of the sentences using the apply() funciton. The lambda-function used first removes punctuation, then applies lower() and split()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calculate token and bigram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>token_count</th>\n",
       "      <th>bigram_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>[wow, loved, this, place]</td>\n",
       "      <td>{'this place': None, 'wow loved': None, 'loved...</td>\n",
       "      <td>{'loved': 1, 'place': 1, 'wow': 1, 'this': 1}</td>\n",
       "      <td>{'this place': 1, 'wow loved': 1, 'loved this'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>[crust, is, not, good]</td>\n",
       "      <td>{'not good': None, 'crust is': None, 'is not':...</td>\n",
       "      <td>{'crust': 1, 'not': 1, 'good': 1, 'is': 1}</td>\n",
       "      <td>{'not good': 1, 'crust is': 1, 'is not': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, tasty, and, the, texture, was, just, nasty]</td>\n",
       "      <td>{'texture was': None, 'not tasty': None, 'just...</td>\n",
       "      <td>{'tasty': 1, 'just': 1, 'nasty': 1, 'was': 1, ...</td>\n",
       "      <td>{'texture was': 1, 'not tasty': 1, 'just nasty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stopped, by, during, the, late, may, bank, ho...</td>\n",
       "      <td>{'may bank': None, 'steve recommendation': Non...</td>\n",
       "      <td>{'steve': 1, 'it': 1, 'rick': 1, 'bank': 1, 'l...</td>\n",
       "      <td>{'may bank': 1, 'steve recommendation': 1, 'an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, selection, on, the, menu, was, great, an...</td>\n",
       "      <td>{'the selection': None, 'great and': None, 'we...</td>\n",
       "      <td>{'menu': 1, 'great': 1, 'was': 1, 'so': 1, 'se...</td>\n",
       "      <td>{'the selection': 1, 'was great': 1, 'were the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, think, food, should, have, flavor, and, te...</td>\n",
       "      <td>{'think food': None, 'were lacking': None, 'fo...</td>\n",
       "      <td>{'flavor': 1, 'i': 1, 'food': 1, 'were': 1, 't...</td>\n",
       "      <td>{'think food': 1, 'were lacking': 1, 'should h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>[appetite, instantly, gone]</td>\n",
       "      <td>{'instantly gone': None, 'appetite instantly':...</td>\n",
       "      <td>{'instantly': 1, 'appetite': 1, 'gone': 1}</td>\n",
       "      <td>{'instantly gone': 1, 'appetite instantly': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[overall, i, was, not, impressed, and, would, ...</td>\n",
       "      <td>{'not impressed': None, 'and would': None, 'wo...</td>\n",
       "      <td>{'i': 1, 'overall': 1, 'go': 1, 'impressed': 1...</td>\n",
       "      <td>{'not impressed': 1, 'and would': 1, 'would no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, whole, experience, was, underwhelming, a...</td>\n",
       "      <td>{'next time': None, 'the whole': None, 'go to'...</td>\n",
       "      <td>{'just': 1, 'i': 1, 'go': 1, 'was': 1, 'to': 1...</td>\n",
       "      <td>{'next time': 1, 'underwhelming and': 1, 'the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[then, as, if, i, hadnt, wasted, enough, of, m...</td>\n",
       "      <td>{'wound by': None, 'took to': None, 'bring the...</td>\n",
       "      <td>{'salt': 1, 'my': 1, 'it': 1, 'to': 1, 'wasted...</td>\n",
       "      <td>{'wound by': 1, 'took to': 1, 'bring the': 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1  \\\n",
       "0                             Wow... Loved this place.  1   \n",
       "1                                   Crust is not good.  0   \n",
       "2            Not tasty and the texture was just nasty.  0   \n",
       "3    Stopped by during the late May bank holiday of...  1   \n",
       "4    The selection on the menu was great and so wer...  1   \n",
       "..                                                 ... ..   \n",
       "995  I think food should have flavor and texture an...  0   \n",
       "996                           Appetite instantly gone.  0   \n",
       "997  Overall I was not impressed and would not go b...  0   \n",
       "998  The whole experience was underwhelming, and I ...  0   \n",
       "999  Then, as if I hadn't wasted enough of my life ...  0   \n",
       "\n",
       "                                                tokens  \\\n",
       "0                            [wow, loved, this, place]   \n",
       "1                               [crust, is, not, good]   \n",
       "2    [not, tasty, and, the, texture, was, just, nasty]   \n",
       "3    [stopped, by, during, the, late, may, bank, ho...   \n",
       "4    [the, selection, on, the, menu, was, great, an...   \n",
       "..                                                 ...   \n",
       "995  [i, think, food, should, have, flavor, and, te...   \n",
       "996                        [appetite, instantly, gone]   \n",
       "997  [overall, i, was, not, impressed, and, would, ...   \n",
       "998  [the, whole, experience, was, underwhelming, a...   \n",
       "999  [then, as, if, i, hadnt, wasted, enough, of, m...   \n",
       "\n",
       "                                               bigrams  \\\n",
       "0    {'this place': None, 'wow loved': None, 'loved...   \n",
       "1    {'not good': None, 'crust is': None, 'is not':...   \n",
       "2    {'texture was': None, 'not tasty': None, 'just...   \n",
       "3    {'may bank': None, 'steve recommendation': Non...   \n",
       "4    {'the selection': None, 'great and': None, 'we...   \n",
       "..                                                 ...   \n",
       "995  {'think food': None, 'were lacking': None, 'fo...   \n",
       "996  {'instantly gone': None, 'appetite instantly':...   \n",
       "997  {'not impressed': None, 'and would': None, 'wo...   \n",
       "998  {'next time': None, 'the whole': None, 'go to'...   \n",
       "999  {'wound by': None, 'took to': None, 'bring the...   \n",
       "\n",
       "                                           token_count  \\\n",
       "0        {'loved': 1, 'place': 1, 'wow': 1, 'this': 1}   \n",
       "1           {'crust': 1, 'not': 1, 'good': 1, 'is': 1}   \n",
       "2    {'tasty': 1, 'just': 1, 'nasty': 1, 'was': 1, ...   \n",
       "3    {'steve': 1, 'it': 1, 'rick': 1, 'bank': 1, 'l...   \n",
       "4    {'menu': 1, 'great': 1, 'was': 1, 'so': 1, 'se...   \n",
       "..                                                 ...   \n",
       "995  {'flavor': 1, 'i': 1, 'food': 1, 'were': 1, 't...   \n",
       "996         {'instantly': 1, 'appetite': 1, 'gone': 1}   \n",
       "997  {'i': 1, 'overall': 1, 'go': 1, 'impressed': 1...   \n",
       "998  {'just': 1, 'i': 1, 'go': 1, 'was': 1, 'to': 1...   \n",
       "999  {'salt': 1, 'my': 1, 'it': 1, 'to': 1, 'wasted...   \n",
       "\n",
       "                                          bigram_count  \n",
       "0    {'this place': 1, 'wow loved': 1, 'loved this'...  \n",
       "1          {'not good': 1, 'crust is': 1, 'is not': 1}  \n",
       "2    {'texture was': 1, 'not tasty': 1, 'just nasty...  \n",
       "3    {'may bank': 1, 'steve recommendation': 1, 'an...  \n",
       "4    {'the selection': 1, 'was great': 1, 'were the...  \n",
       "..                                                 ...  \n",
       "995  {'think food': 1, 'were lacking': 1, 'should h...  \n",
       "996     {'instantly gone': 1, 'appetite instantly': 1}  \n",
       "997  {'not impressed': 1, 'and would': 1, 'would no...  \n",
       "998  {'next time': 1, 'underwhelming and': 1, 'the ...  \n",
       "999  {'wound by': 1, 'took to': 1, 'bring the': 1, ...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokens_to_token_freq(tokens):\n",
    "    dictionary = dict.fromkeys(tokens)\n",
    "    for i in list(dictionary):\n",
    "        count = 0\n",
    "        for j in tokens:\n",
    "            if j==i:\n",
    "                count += 1\n",
    "        dictionary[i] = count\n",
    "    return dictionary\n",
    "\n",
    "def bigram_list(sent):\n",
    "    bigrams=[]\n",
    "    for i in range(0, len(sent) - 1):\n",
    "        bigrams.append(sent[i] + ' ' + sent[i + 1])\n",
    "    return bigrams\n",
    "\n",
    "df = df.assign(bigrams = df[\"tokens\"].apply(lambda x : dict.fromkeys(bigram_list(x))))\n",
    "df = df.assign(token_count = df[\"tokens\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "df = df.assign(bigram_count = df[\"bigrams\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Print the 10 most frequent bigrams and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('on the', 23), ('the service', 24), ('service was', 26), ('and i', 27), ('of the', 29), ('food was', 32), ('and the', 38), ('it was', 41), ('the food', 47), ('this place', 72)]\n"
     ]
    }
   ],
   "source": [
    "def frequency(column):\n",
    "    frequency = dict(functools.reduce(operator.add, \n",
    "         map(collections.Counter, column)))\n",
    "    frequency = sorted(frequency.items(), key=lambda x: x[1])\n",
    "    return(list(frequency))\n",
    "\n",
    "bigrams = frequency(df[\"bigram_count\"])\n",
    "print(bigrams[len(bigrams) - 10 : ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.) Print a bigram matrix from the 10 most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  4.  0.  0. 10.  0.  0.  0. 29.]\n",
      " [ 0.  0.  0. 10.  2.  1. 41.  2.  3.  0.]\n",
      " [ 0.  0.  0. 20.  1.  0. 10.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  1. 16.  1.  0.  0. 10.]\n",
      " [ 0.  1.  2.  0.  0.  3.  0.  0.  1. 11.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0. 21.  0.  0.  0. 13.]\n",
      " [ 0.  0.  0.  0.  0.  0. 19.  0.  0.  0.]\n",
      " [ 0. 17.  4.  0.  0.  3.  5. 27.  0. 38.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "token_frequency = frequency(df[\"token_count\"])\n",
    "\n",
    "def bigram_matrix(token_list, bigram_frequencies):\n",
    "    matrix = np.zeros((len(token_list), len(token_list)))\n",
    "    for i in range(0, len(matrix)):\n",
    "        for j in range(0, len(matrix)):\n",
    "            key_str = token_list[i][0] + \" \" + token_list[j][0]\n",
    "            bigram_frequencies = dict(bigram_frequencies)\n",
    "            if key_str in bigram_frequencies:\n",
    "                matrix[i, j] = bigram_frequencies[key_str]          \n",
    "    return matrix\n",
    "\n",
    "print(bigram_matrix(token_frequency[len(token_frequency) - 10 : ], bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code of Task 1:\n",
    "\n",
    "a.) Extract top 20 most frequent bigrams of all positive and negative documents in folder polarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and his', 4), ('from a', 4), ('youve got', 4), ('and the', 5), ('the first', 5), ('the movie', 5), ('it is', 5), ('this film', 5), ('as the', 5), ('from the', 5), ('is a', 6), ('to be', 7), ('is the', 7), ('with a', 7), ('the film', 8), ('by the', 8), ('with the', 8), ('the shark', 9), ('of the', 14), ('in the', 24)]\n",
      "                                                    0  \\\n",
      "0   synopsis : a mentally unstable man undergoing ...   \n",
      "1   unsuccessfully attempting to gain the woman's ...   \n",
      "2   comments : stalked is yet another in a seeming...   \n",
      "3   their proliferation may be due in part to the ...   \n",
      "4   stalked wavers slightly from the norm in one r...   \n",
      "..                                                ...   \n",
      "30  it also wrapped production two years ago and h...   \n",
      "31                               whatever . . . skip    \n",
      "32                                              it !    \n",
      "33                       where's joblo coming from ?    \n",
      "34  a nightmare of elm street 3 ( 7/10 ) - blair w...   \n",
      "\n",
      "                                               tokens  \n",
      "0   [synopsis, a, mentally, unstable, man, undergo...  \n",
      "1   [unsuccessfully, attempting, to, gain, the, wo...  \n",
      "2   [comments, stalked, is, yet, another, in, a, s...  \n",
      "3   [their, proliferation, may, be, due, in, part,...  \n",
      "4   [stalked, wavers, slightly, from, the, norm, i...  \n",
      "..                                                ...  \n",
      "30  [it, also, wrapped, production, two, years, ag...  \n",
      "31                                   [whatever, skip]  \n",
      "32                                               [it]  \n",
      "33                      [wheres, joblo, coming, from]  \n",
      "34  [a, nightmare, of, elm, street, 3, 710, blair,...  \n",
      "\n",
      "[127 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'bigrams'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bigrams'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3005827dc14d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mneg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_bigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mneg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtokens_to_token_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mneg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bigrams\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtokens_to_token_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# print(neg_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# neg_bigrams = frequency(neg_df[\"bigram_count\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bigrams'"
     ]
    }
   ],
   "source": [
    "pos_docs = os.listdir(\"./polarity/pos\")\n",
    "pos_df = read_file(\"./polarity/pos/\" + pos_docs[0])\n",
    "for doc in pos_docs[1:]:\n",
    "    pos_df = pos_df.append(read_file(\"./polarity/pos/\" + doc))\n",
    "\n",
    "pos_df = pos_df.assign(bigrams = pos_df[\"tokens\"].apply(lambda x : dict.fromkeys(bigram_list(x))))\n",
    "pos_df = pos_df.assign(token_count = pos_df[\"tokens\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "pos_df = pos_df.assign(bigram_count = pos_df[\"bigrams\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "\n",
    "pos_bigrams = frequency(pos_df[\"bigram_count\"])\n",
    "print(pos_bigrams[len(pos_bigrams) - 20:])\n",
    "\n",
    "neg_docs = os.listdir(\"./polarity/neg\")\n",
    "neg_df = read_file(\"./polarity/neg/\" + neg_docs[0])\n",
    "for doc in neg_docs[1:]:\n",
    "    neg_df = neg_df.append(read_file(\"./polarity/neg/\" + doc))\n",
    "\n",
    "print(neg_df)\n",
    "neg_df = neg_df.assign(neg_bigrams = neg_df[\"tokens\"].apply(lambda x : dict.fromkeys(bigram_list(x))))\n",
    "neg_df = neg_df.assign(token_count = neg_df[\"tokens\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "neg_df = neg_df.assign(bigram_count = neg_df[\"bigrams\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "# print(neg_df)\n",
    "# neg_bigrams = frequency(neg_df[\"bigram_count\"])\n",
    "# print(neg_bigrams[len(neg_bigrams) - 20:])\n",
    "# print(neg_df[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.) Calculate their probability as aforementioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.) Write a script to save in a file the most probably bigrams of both classes and their probabilites separated by tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.) Compare the output of both classes and write an analysis of your obersvations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
