{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Tokenize the sentences in fileyelp_polarity.txt, removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-68da124277e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctoolsoperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'collection'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import collection\n",
    "import functools\n",
    "import operator \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def read_file(filename):\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", header=None)\n",
    "    df = df.assign(tokens =df[0].apply(lambda x : x.translate(str.maketrans('', '', string.punctuation)).lower().split()))\n",
    "    return df\n",
    "\n",
    "df = read_file(\"yelp_polarity.txt\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is read using the function read_file. Read_file uses pandas to read the file and saves it in a dataframe. A function is then applied to all of the sentences using the apply() funciton and saved in a new column titled \"tokens\". The lambda-function used first removes punctuation, then applies lower() and split(). This ensures that puntuaction is not included in the tokens and removal of capitalizatrion does not lead to two tokens being considered different when written using a different case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calculate token and bigram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>token_count</th>\n",
       "      <th>bigram_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>[wow, loved, this, place]</td>\n",
       "      <td>{'loved this': None, 'this place': None, 'wow ...</td>\n",
       "      <td>{'loved': 1, 'place': 1, 'wow': 1, 'this': 1}</td>\n",
       "      <td>{'loved this': 1, 'this place': 1, 'wow loved'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>[crust, is, not, good]</td>\n",
       "      <td>{'crust is': None, 'is not': None, 'not good':...</td>\n",
       "      <td>{'not': 1, 'good': 1, 'crust': 1, 'is': 1}</td>\n",
       "      <td>{'crust is': 1, 'is not': 1, 'not good': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, tasty, and, the, texture, was, just, nasty]</td>\n",
       "      <td>{'was just': None, 'and the': None, 'tasty and...</td>\n",
       "      <td>{'the': 1, 'tasty': 1, 'just': 1, 'nasty': 1, ...</td>\n",
       "      <td>{'was just': 1, 'and the': 1, 'tasty and': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stopped, by, during, the, late, may, bank, ho...</td>\n",
       "      <td>{'may bank': None, 'rick steve': None, 'late m...</td>\n",
       "      <td>{'stopped': 1, 'during': 1, 'the': 1, 'may': 1...</td>\n",
       "      <td>{'may bank': 1, 'rick steve': 1, 'late may': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, selection, on, the, menu, was, great, an...</td>\n",
       "      <td>{'the prices': None, 'so were': None, 'was gre...</td>\n",
       "      <td>{'so': 1, 'prices': 1, 'menu': 1, 'selection':...</td>\n",
       "      <td>{'the prices': 1, 'was great': 1, 'great and':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, think, food, should, have, flavor, and, te...</td>\n",
       "      <td>{'food should': None, 'i think': None, 'and te...</td>\n",
       "      <td>{'texture': 1, 'think': 1, 'i': 1, 'and': 2, '...</td>\n",
       "      <td>{'food should': 1, 'i think': 1, 'and texture'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>[appetite, instantly, gone]</td>\n",
       "      <td>{'appetite instantly': None, 'instantly gone':...</td>\n",
       "      <td>{'appetite': 1, 'instantly': 1, 'gone': 1}</td>\n",
       "      <td>{'appetite instantly': 1, 'instantly gone': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[overall, i, was, not, impressed, and, would, ...</td>\n",
       "      <td>{'was not': None, 'would not': None, 'overall ...</td>\n",
       "      <td>{'would': 1, 'i': 1, 'back': 1, 'go': 1, 'over...</td>\n",
       "      <td>{'was not': 1, 'not go': 1, 'would not': 1, 'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, whole, experience, was, underwhelming, a...</td>\n",
       "      <td>{'just go': None, 'i think': None, 'ninja sush...</td>\n",
       "      <td>{'go': 1, 'next': 1, 'the': 1, 'sushi': 1, 'ex...</td>\n",
       "      <td>{'just go': 1, 'i think': 1, 'whole experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[then, as, if, i, hadnt, wasted, enough, of, m...</td>\n",
       "      <td>{'drawing out': None, 'salt in': None, 'poured...</td>\n",
       "      <td>{'took': 1, 'then': 1, 'wound': 1, 'life': 1, ...</td>\n",
       "      <td>{'drawing out': 1, 'salt in': 1, 'poured salt'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1  \\\n",
       "0                             Wow... Loved this place.  1   \n",
       "1                                   Crust is not good.  0   \n",
       "2            Not tasty and the texture was just nasty.  0   \n",
       "3    Stopped by during the late May bank holiday of...  1   \n",
       "4    The selection on the menu was great and so wer...  1   \n",
       "..                                                 ... ..   \n",
       "995  I think food should have flavor and texture an...  0   \n",
       "996                           Appetite instantly gone.  0   \n",
       "997  Overall I was not impressed and would not go b...  0   \n",
       "998  The whole experience was underwhelming, and I ...  0   \n",
       "999  Then, as if I hadn't wasted enough of my life ...  0   \n",
       "\n",
       "                                                tokens  \\\n",
       "0                            [wow, loved, this, place]   \n",
       "1                               [crust, is, not, good]   \n",
       "2    [not, tasty, and, the, texture, was, just, nasty]   \n",
       "3    [stopped, by, during, the, late, may, bank, ho...   \n",
       "4    [the, selection, on, the, menu, was, great, an...   \n",
       "..                                                 ...   \n",
       "995  [i, think, food, should, have, flavor, and, te...   \n",
       "996                        [appetite, instantly, gone]   \n",
       "997  [overall, i, was, not, impressed, and, would, ...   \n",
       "998  [the, whole, experience, was, underwhelming, a...   \n",
       "999  [then, as, if, i, hadnt, wasted, enough, of, m...   \n",
       "\n",
       "                                               bigrams  \\\n",
       "0    {'loved this': None, 'this place': None, 'wow ...   \n",
       "1    {'crust is': None, 'is not': None, 'not good':...   \n",
       "2    {'was just': None, 'and the': None, 'tasty and...   \n",
       "3    {'may bank': None, 'rick steve': None, 'late m...   \n",
       "4    {'the prices': None, 'so were': None, 'was gre...   \n",
       "..                                                 ...   \n",
       "995  {'food should': None, 'i think': None, 'and te...   \n",
       "996  {'appetite instantly': None, 'instantly gone':...   \n",
       "997  {'was not': None, 'would not': None, 'overall ...   \n",
       "998  {'just go': None, 'i think': None, 'ninja sush...   \n",
       "999  {'drawing out': None, 'salt in': None, 'poured...   \n",
       "\n",
       "                                           token_count  \\\n",
       "0        {'loved': 1, 'place': 1, 'wow': 1, 'this': 1}   \n",
       "1           {'not': 1, 'good': 1, 'crust': 1, 'is': 1}   \n",
       "2    {'the': 1, 'tasty': 1, 'just': 1, 'nasty': 1, ...   \n",
       "3    {'stopped': 1, 'during': 1, 'the': 1, 'may': 1...   \n",
       "4    {'so': 1, 'prices': 1, 'menu': 1, 'selection':...   \n",
       "..                                                 ...   \n",
       "995  {'texture': 1, 'think': 1, 'i': 1, 'and': 2, '...   \n",
       "996         {'appetite': 1, 'instantly': 1, 'gone': 1}   \n",
       "997  {'would': 1, 'i': 1, 'back': 1, 'go': 1, 'over...   \n",
       "998  {'go': 1, 'next': 1, 'the': 1, 'sushi': 1, 'ex...   \n",
       "999  {'took': 1, 'then': 1, 'wound': 1, 'life': 1, ...   \n",
       "\n",
       "                                          bigram_count  \n",
       "0    {'loved this': 1, 'this place': 1, 'wow loved'...  \n",
       "1          {'crust is': 1, 'is not': 1, 'not good': 1}  \n",
       "2    {'was just': 1, 'and the': 1, 'tasty and': 1, ...  \n",
       "3    {'may bank': 1, 'rick steve': 1, 'late may': 1...  \n",
       "4    {'the prices': 1, 'was great': 1, 'great and':...  \n",
       "..                                                 ...  \n",
       "995  {'food should': 1, 'i think': 1, 'and texture'...  \n",
       "996     {'appetite instantly': 1, 'instantly gone': 1}  \n",
       "997  {'was not': 1, 'not go': 1, 'would not': 1, 'i...  \n",
       "998  {'just go': 1, 'i think': 1, 'whole experience...  \n",
       "999  {'drawing out': 1, 'salt in': 1, 'poured salt'...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ngram_to_ngram_freq(tokens):\n",
    "    dictionary = dict.fromkeys(tokens)\n",
    "    for i in list(dictionary):\n",
    "        count = 0\n",
    "        for j in tokens:\n",
    "            if j==i:\n",
    "                count += 1\n",
    "        dictionary[i] = count\n",
    "    return dictionary\n",
    "\n",
    "def bigram_list(sent):\n",
    "    bigrams=[]\n",
    "    for i in range(0, len(sent) - 1):\n",
    "        bigrams.append(sent[i] + ' ' + sent[i + 1])\n",
    "    return bigrams\n",
    "\n",
    "df = df.assign(bigrams = df[\"tokens\"].apply(lambda x : dict.fromkeys(bigram_list(x))))\n",
    "df = df.assign(token_count = df[\"tokens\"].apply(lambda x : ngram_to_ngram_freq(x)))\n",
    "df = df.assign(bigram_count = df[\"bigrams\"].apply(lambda x : ngram_to_ngram_freq(x)))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ngram_to_ngram_freq takes a list of ngrams as a paramater and returns a dictionary with distinct ngrams as keys and the individual frequencies in the list of ngrams. \n",
    "\n",
    "The function bigram_list determines all the bigrams that occur in a given tokenized sentence (that retains the word order of the untokenized sentence!) passed as a parameter. This is done by iterating through the tokens and saving all bigrams.\n",
    "\n",
    "To complete the task, the column \"bigrams\" of all bigrams that exist in each sentence is added to the dataframe from the first task by applying the bigram_list function to the column \"tokens\". Then two additional coloums are created, \"token_count\" and \"bigram_count\" which contain the result of ngram_to_ngram_freq after being passed the \"tokens\" and the \"bigrams\" column respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Print the 10 most frequent bigrams and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('on the', 23), ('the service', 24), ('service was', 26), ('and i', 27), ('of the', 29), ('food was', 32), ('and the', 38), ('it was', 41), ('the food', 47), ('this place', 72)]\n"
     ]
    }
   ],
   "source": [
    "def frequency(column):\n",
    "    frequency = dict(functools.reduce(operator.add, map(collections.Counter, column)))\n",
    "    frequency = sorted(frequency.items(), key=lambda x: x[1])\n",
    "    return(list(frequency))\n",
    "\n",
    "bigrams = frequency(df[\"bigram_count\"])\n",
    "print(bigrams[len(bigrams) - 10 : ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function freuquency takes the dataframe column passed as a parameter, which has to consist of dictionary of words and their freuquecies per row, and calculates the overall frequency in the whole dataframe sorted in asecneding order by the frequency. The result is a list of bigrams and their frequency.\n",
    "\n",
    "This function is called on he colum \"bigram_count\" created in the previous task. To find the ten most frequent bigrms the last ten elements of the list are printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.) Print a bigram matrix from the 10 most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>it</th>\n",
       "      <th>and</th>\n",
       "      <th>the</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "      <th>of</th>\n",
       "      <th>i</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        it  and   the    is     a  this   was   of     i   to\n",
       "it     0.0  3.0   0.0  10.0   1.0   0.0  41.0  0.0   2.0  2.0\n",
       "and   17.0  0.0  38.0   0.0   3.0   4.0   5.0  0.0  27.0  0.0\n",
       "the    0.0  0.0   0.0   0.0   0.0   0.0   0.0  0.0   0.0  0.0\n",
       "is     0.0  0.0  10.0   0.0  16.0   0.0   1.0  1.0   0.0  1.0\n",
       "a      0.0  0.0   0.0   0.0   0.0   0.0   0.0  0.0   0.0  0.0\n",
       "this   0.0  0.0   0.0  20.0   0.0   0.0  10.0  0.0   0.0  1.0\n",
       "was    0.0  0.0  13.0   0.0  21.0   0.0   0.0  0.0   0.0  0.0\n",
       "of     1.0  0.0  29.0   0.0  10.0   4.0   0.0  0.0   0.0  0.0\n",
       "i      0.0  0.0   0.0   0.0   0.0   0.0  19.0  0.0   0.0  0.0\n",
       "to     1.0  1.0  11.0   0.0   3.0   2.0   0.0  0.0   0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_frequency = frequency(df[\"token_count\"])\n",
    "def bigram_matrix(token_list, bigram_frequencies):\n",
    "    matrix = np.zeros((len(token_list), len(token_list)))\n",
    "    bigram_frequencies = dict(bigram_frequencies)\n",
    "    for i in range(0, len(matrix)):\n",
    "        for j in range(0, len(matrix)):\n",
    "            key_str = token_list[i] + \" \" + token_list[j]\n",
    "            if key_str in bigram_frequencies:\n",
    "                matrix[i, j] = bigram_frequencies[key_str] \n",
    "    return matrix\n",
    "freq_matrix = bigram_matrix(list(dict(token_frequency[len(token_frequency) - 10 : ]).keys()), bigrams)\n",
    "token_strings = dict(token_frequency[len(token_frequency) - 10 : ]).keys()\n",
    "display(pd.DataFrame(freq_matrix, token_strings, token_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the most frequent tokens are determined using the previously implemented frequency fucntion.\n",
    "\n",
    "Then the function bigram_matrix is defined. This d=function takes a list of tokens and a list of all possible bigrams wit their respective frequencies as input. A two dimensional array (matrix) is createn with the demensions given by the number of tokens in the token_list. With a doulbe for-loop the function then iterates over all possible bigrams that can be created with the the token_list and saves the frequency of that bigram in the matrix.\n",
    "\n",
    "This function is called and given the ten most frequent tokens as well as the previously created dictionary bigrams. The return matrix is then read into an new pandas Data frame and displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code of Task 1:\n",
    "\n",
    "a.) Extract top 20 most frequent bigrams of all positive and negative documents in folder polarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('for the', 4), ('there is', 4), ('the story', 4), ('it is', 5), ('the first', 5), ('as the', 5), ('the movie', 5), ('this film', 5), ('from the', 5), ('and the', 5), ('is a', 6), ('is the', 7), ('with a', 7), ('to be', 7), ('by the', 8), ('with the', 8), ('the film', 8), ('the shark', 9), ('of the', 14), ('in the', 24)]\n",
      "\n",
      "[('they are', 4), ('the story', 4), ('is not', 5), ('from the', 5), ('of a', 5), ('and the', 5), ('by the', 5), ('it is', 6), ('in a', 6), ('with the', 6), ('the movie', 6), ('for the', 6), ('film is', 6), ('of course', 6), ('on the', 7), ('in the', 8), ('to be', 8), ('that the', 8), ('the film', 9), ('of the', 10)]\n"
     ]
    }
   ],
   "source": [
    "pos_docs = os.listdir(\"./polarity/pos\")\n",
    "pos_df = read_file(\"./polarity/pos/\" + pos_docs[0])\n",
    "for doc in pos_docs[1:]:\n",
    "    pos_df = pos_df.append(read_file(\"./polarity/pos/\" + doc))\n",
    "\n",
    "pos_df = pos_df.assign(bigrams = pos_df[\"tokens\"].apply(lambda x : dict.fromkeys(bigram_list(x))))\n",
    "pos_df = pos_df.assign(token_count = pos_df[\"tokens\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "pos_df = pos_df.assign(bigram_count = pos_df[\"bigrams\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "\n",
    "pos_bigrams = frequency(pos_df[\"bigram_count\"])\n",
    "print(pos_bigrams[len(pos_bigrams) - 20:])\n",
    "\n",
    "neg_docs = os.listdir(\"./polarity/neg\")\n",
    "neg_df = read_file(\"./polarity/neg/\" + neg_docs[0])\n",
    "for doc in neg_docs[1:]:\n",
    "    neg_df = neg_df.append(read_file(\"./polarity/neg/\" + doc))\n",
    "\n",
    "print()\n",
    "neg_df = neg_df.assign(neg_bigrams = neg_df[\"tokens\"].apply(lambda x : dict.fromkeys(bigram_list(x))))\n",
    "neg_df = neg_df.assign(token_count = neg_df[\"tokens\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "neg_df = neg_df.assign(bigram_count = neg_df[\"neg_bigrams\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "# print(neg_df)\n",
    "neg_bigrams = frequency(neg_df[\"bigram_count\"])\n",
    "print(neg_bigrams[len(neg_bigrams) - 20:])\n",
    "# print(neg_df[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.) Calculate their probability as aforementioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Bigram Probabilities\n",
      "[('they are', 0.0016326530612244899), ('the story', 0.0016326530612244899), ('is not', 0.0020408163265306124), ('from the', 0.0020408163265306124), ('of a', 0.0020408163265306124), ('and the', 0.0020408163265306124), ('by the', 0.0020408163265306124), ('it is', 0.0024489795918367346), ('in a', 0.0024489795918367346), ('with the', 0.0024489795918367346), ('the movie', 0.0024489795918367346), ('for the', 0.0024489795918367346), ('film is', 0.0024489795918367346), ('of course', 0.0024489795918367346), ('on the', 0.002857142857142857), ('in the', 0.0032653061224489797), ('to be', 0.0032653061224489797), ('that the', 0.0032653061224489797), ('the film', 0.003673469387755102), ('of the', 0.004081632653061225)]\n",
      "\n",
      "Positive Bigram Probabilities\n",
      "[('for the', 0.001234186979327368), ('there is', 0.001234186979327368), ('the story', 0.001234186979327368), ('it is', 0.0015427337241592102), ('the first', 0.0015427337241592102), ('as the', 0.0015427337241592102), ('the movie', 0.0015427337241592102), ('this film', 0.0015427337241592102), ('from the', 0.0015427337241592102), ('and the', 0.0015427337241592102), ('is a', 0.0018512804689910522), ('is the', 0.0021598272138228943), ('with a', 0.0021598272138228943), ('to be', 0.0021598272138228943), ('by the', 0.002468373958654736), ('with the', 0.002468373958654736), ('the film', 0.002468373958654736), ('the shark', 0.002776920703486578), ('of the', 0.004319654427645789), ('in the', 0.007405121875964209)]\n"
     ]
    }
   ],
   "source": [
    "top_pos_bigrams = pos_bigrams[len(pos_bigrams) - 20:]\n",
    "top_neg_bigrams = neg_bigrams[len(neg_bigrams) - 20:]\n",
    "\n",
    "def calculate_probabilty(all_bigrams, bigrams):\n",
    "    probability = []\n",
    "    for x in bigrams:\n",
    "        probability.append((x[0], x[1]/sum(dict(all_bigrams).values())))\n",
    "    return probability\n",
    "\n",
    "neg_probability = calculate_probabilty(neg_bigrams, top_neg_bigrams)\n",
    "print(\"Negative Bigram Probabilities\")\n",
    "print(neg_probability)\n",
    "print()\n",
    "\n",
    "pos_probability = calculate_probabilty(pos_bigrams, top_pos_bigrams)\n",
    "print(\"Positive Bigram Probabilities\")\n",
    "print(pos_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.) Write a script to save in a file the most probable bigrams of both classes and their probabilites separated by tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg prob</th>\n",
       "      <th>pos prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and the</th>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as the</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by the</th>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.002468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film is</th>\n",
       "      <td>0.002449</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for the</th>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from the</th>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in a</th>\n",
       "      <td>0.002449</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in the</th>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.007405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is a</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is not</th>\n",
       "      <td>0.002041</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is the</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it is</th>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of a</th>\n",
       "      <td>0.002041</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of course</th>\n",
       "      <td>0.002449</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of the</th>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.004320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on the</th>\n",
       "      <td>0.002857</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that the</th>\n",
       "      <td>0.003265</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the film</th>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.002468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the movie</th>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the shark</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the story</th>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>there is</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they are</th>\n",
       "      <td>0.001633</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this film</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to be</th>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.002160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with a</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with the</th>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.002468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           neg prob  pos prob\n",
       "and the    0.002041  0.001543\n",
       "as the          NaN  0.001543\n",
       "by the     0.002041  0.002468\n",
       "film is    0.002449       NaN\n",
       "for the    0.002449  0.001234\n",
       "from the   0.002041  0.001543\n",
       "in a       0.002449       NaN\n",
       "in the     0.003265  0.007405\n",
       "is a            NaN  0.001851\n",
       "is not     0.002041       NaN\n",
       "is the          NaN  0.002160\n",
       "it is      0.002449  0.001543\n",
       "of a       0.002041       NaN\n",
       "of course  0.002449       NaN\n",
       "of the     0.004082  0.004320\n",
       "on the     0.002857       NaN\n",
       "that the   0.003265       NaN\n",
       "the film   0.003673  0.002468\n",
       "the first       NaN  0.001543\n",
       "the movie  0.002449  0.001543\n",
       "the shark       NaN  0.002777\n",
       "the story  0.001633  0.001234\n",
       "there is        NaN  0.001234\n",
       "they are   0.001633       NaN\n",
       "this film       NaN  0.001543\n",
       "to be      0.003265  0.002160\n",
       "with a          NaN  0.002160\n",
       "with the   0.002449  0.002468"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neg_prob = dict(neg_probability)\n",
    "pos_prob = dict(pos_probability)\n",
    "\n",
    "df_neg = pd.DataFrame(neg_prob.values(),neg_prob.keys())\n",
    "df_neg.columns = ['neg prob']\n",
    "df_pos =  pd.DataFrame(pos_prob.values(), pos_prob.keys())\n",
    "df_pos.columns = ['pos prob']\n",
    "df_total = pd.concat([df_neg, df_pos], axis=1, sort=True)\n",
    "\n",
    "display(df_total)\n",
    "df.to_csv(\"test1.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.) Compare the output of both classes and write an analysis of your obersvations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the 10 most frequent bigrams in pos aren't the same top 10 in neg, even though several are in the top 10 of both. It is also clear that many of the bigrams that made into the top 10 of both datasets consit of the most common English words such as \"with the\" or \"and the\". More rare bigrams such as \"the shark\" or \"film is\" only appear in the top 10 of one of the datasets. However, the frequencies of the top 10 bigrams don't reveal a lot, since the vast majority consist of very common English words/bigrams that can be used in almost every context. The fact that the word \"film\" appears in multiple bigrams in both datasets and \"movie\" also apears in both datasets, suggest that the dataset has something to do with ratings of a movie/film. Considering this, it is also worht noting that bigrams with very similiar meaning, such as \"the film\" and \"the movie\" are not combined and don't have a combined frequency even though they are synonymous. This show us that further methods are needed to really gain meaningful data from this (or other) datasets. It is also worth noting that even the highest frequencies are fairy low, but this isn't surprising when considering all the possible bigrams contained in a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
