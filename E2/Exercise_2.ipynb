{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Tokenize the sentences in fileyelp_polarity.txt, removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>[wow, loved, this, place]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>[crust, is, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, tasty, and, the, texture, was, just, nasty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stopped, by, during, the, late, may, bank, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, selection, on, the, menu, was, great, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, think, food, should, have, flavor, and, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>[appetite, instantly, gone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[overall, i, was, not, impressed, and, would, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, whole, experience, was, underwhelming, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[then, as, if, i, hadnt, wasted, enough, of, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1  \\\n",
       "0                             Wow... Loved this place.  1   \n",
       "1                                   Crust is not good.  0   \n",
       "2            Not tasty and the texture was just nasty.  0   \n",
       "3    Stopped by during the late May bank holiday of...  1   \n",
       "4    The selection on the menu was great and so wer...  1   \n",
       "..                                                 ... ..   \n",
       "995  I think food should have flavor and texture an...  0   \n",
       "996                           Appetite instantly gone.  0   \n",
       "997  Overall I was not impressed and would not go b...  0   \n",
       "998  The whole experience was underwhelming, and I ...  0   \n",
       "999  Then, as if I hadn't wasted enough of my life ...  0   \n",
       "\n",
       "                                                tokens  \n",
       "0                            [wow, loved, this, place]  \n",
       "1                               [crust, is, not, good]  \n",
       "2    [not, tasty, and, the, texture, was, just, nasty]  \n",
       "3    [stopped, by, during, the, late, may, bank, ho...  \n",
       "4    [the, selection, on, the, menu, was, great, an...  \n",
       "..                                                 ...  \n",
       "995  [i, think, food, should, have, flavor, and, te...  \n",
       "996                        [appetite, instantly, gone]  \n",
       "997  [overall, i, was, not, impressed, and, would, ...  \n",
       "998  [the, whole, experience, was, underwhelming, a...  \n",
       "999  [then, as, if, i, hadnt, wasted, enough, of, m...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "import collections, functools, operator \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def read_file(filename):\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", header=None)\n",
    "    df = df.assign(tokens =df[0].apply(lambda x : x.translate(str.maketrans('', '', string.punctuation)).lower().split()))\n",
    "    return df\n",
    "\n",
    "df = read_file(\"yelp_polarity.txt\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is read using pandas and saved in the dataframe df. A function is then applied to all of the sentences using the apply() funciton. The lambda-function used first removes punctuation, then applies lower() and split()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calculate token and bigram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>token_count</th>\n",
       "      <th>bigram_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>[wow, loved, this, place]</td>\n",
       "      <td>{'wow loved': None, 'loved this': None, 'this ...</td>\n",
       "      <td>{'wow': 1, 'loved': 1, 'this': 1, 'place': 1}</td>\n",
       "      <td>{'wow loved': 1, 'loved this': 1, 'this place'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>[crust, is, not, good]</td>\n",
       "      <td>{'crust is': None, 'is not': None, 'not good':...</td>\n",
       "      <td>{'crust': 1, 'is': 1, 'not': 1, 'good': 1}</td>\n",
       "      <td>{'crust is': 1, 'is not': 1, 'not good': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>[not, tasty, and, the, texture, was, just, nasty]</td>\n",
       "      <td>{'not tasty': None, 'tasty and': None, 'and th...</td>\n",
       "      <td>{'not': 1, 'tasty': 1, 'and': 1, 'the': 1, 'te...</td>\n",
       "      <td>{'not tasty': 1, 'tasty and': 1, 'and the': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>[stopped, by, during, the, late, may, bank, ho...</td>\n",
       "      <td>{'stopped by': None, 'by during': None, 'durin...</td>\n",
       "      <td>{'stopped': 1, 'by': 1, 'during': 1, 'the': 1,...</td>\n",
       "      <td>{'stopped by': 1, 'by during': 1, 'during the'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, selection, on, the, menu, was, great, an...</td>\n",
       "      <td>{'the selection': None, 'selection on': None, ...</td>\n",
       "      <td>{'the': 3, 'selection': 1, 'on': 1, 'menu': 1,...</td>\n",
       "      <td>{'the selection': 1, 'selection on': 1, 'on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, think, food, should, have, flavor, and, te...</td>\n",
       "      <td>{'i think': None, 'think food': None, 'food sh...</td>\n",
       "      <td>{'i': 1, 'think': 1, 'food': 1, 'should': 1, '...</td>\n",
       "      <td>{'i think': 1, 'think food': 1, 'food should':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>[appetite, instantly, gone]</td>\n",
       "      <td>{'appetite instantly': None, 'instantly gone':...</td>\n",
       "      <td>{'appetite': 1, 'instantly': 1, 'gone': 1}</td>\n",
       "      <td>{'appetite instantly': 1, 'instantly gone': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[overall, i, was, not, impressed, and, would, ...</td>\n",
       "      <td>{'overall i': None, 'i was': None, 'was not': ...</td>\n",
       "      <td>{'overall': 1, 'i': 1, 'was': 1, 'not': 2, 'im...</td>\n",
       "      <td>{'overall i': 1, 'i was': 1, 'was not': 1, 'no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, whole, experience, was, underwhelming, a...</td>\n",
       "      <td>{'the whole': None, 'whole experience': None, ...</td>\n",
       "      <td>{'the': 1, 'whole': 1, 'experience': 1, 'was':...</td>\n",
       "      <td>{'the whole': 1, 'whole experience': 1, 'exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[then, as, if, i, hadnt, wasted, enough, of, m...</td>\n",
       "      <td>{'then as': None, 'as if': None, 'if i': None,...</td>\n",
       "      <td>{'then': 1, 'as': 1, 'if': 1, 'i': 1, 'hadnt':...</td>\n",
       "      <td>{'then as': 1, 'as if': 1, 'if i': 1, 'i hadnt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1  \\\n",
       "0                             Wow... Loved this place.  1   \n",
       "1                                   Crust is not good.  0   \n",
       "2            Not tasty and the texture was just nasty.  0   \n",
       "3    Stopped by during the late May bank holiday of...  1   \n",
       "4    The selection on the menu was great and so wer...  1   \n",
       "..                                                 ... ..   \n",
       "995  I think food should have flavor and texture an...  0   \n",
       "996                           Appetite instantly gone.  0   \n",
       "997  Overall I was not impressed and would not go b...  0   \n",
       "998  The whole experience was underwhelming, and I ...  0   \n",
       "999  Then, as if I hadn't wasted enough of my life ...  0   \n",
       "\n",
       "                                                tokens  \\\n",
       "0                            [wow, loved, this, place]   \n",
       "1                               [crust, is, not, good]   \n",
       "2    [not, tasty, and, the, texture, was, just, nasty]   \n",
       "3    [stopped, by, during, the, late, may, bank, ho...   \n",
       "4    [the, selection, on, the, menu, was, great, an...   \n",
       "..                                                 ...   \n",
       "995  [i, think, food, should, have, flavor, and, te...   \n",
       "996                        [appetite, instantly, gone]   \n",
       "997  [overall, i, was, not, impressed, and, would, ...   \n",
       "998  [the, whole, experience, was, underwhelming, a...   \n",
       "999  [then, as, if, i, hadnt, wasted, enough, of, m...   \n",
       "\n",
       "                                               bigrams  \\\n",
       "0    {'wow loved': None, 'loved this': None, 'this ...   \n",
       "1    {'crust is': None, 'is not': None, 'not good':...   \n",
       "2    {'not tasty': None, 'tasty and': None, 'and th...   \n",
       "3    {'stopped by': None, 'by during': None, 'durin...   \n",
       "4    {'the selection': None, 'selection on': None, ...   \n",
       "..                                                 ...   \n",
       "995  {'i think': None, 'think food': None, 'food sh...   \n",
       "996  {'appetite instantly': None, 'instantly gone':...   \n",
       "997  {'overall i': None, 'i was': None, 'was not': ...   \n",
       "998  {'the whole': None, 'whole experience': None, ...   \n",
       "999  {'then as': None, 'as if': None, 'if i': None,...   \n",
       "\n",
       "                                           token_count  \\\n",
       "0        {'wow': 1, 'loved': 1, 'this': 1, 'place': 1}   \n",
       "1           {'crust': 1, 'is': 1, 'not': 1, 'good': 1}   \n",
       "2    {'not': 1, 'tasty': 1, 'and': 1, 'the': 1, 'te...   \n",
       "3    {'stopped': 1, 'by': 1, 'during': 1, 'the': 1,...   \n",
       "4    {'the': 3, 'selection': 1, 'on': 1, 'menu': 1,...   \n",
       "..                                                 ...   \n",
       "995  {'i': 1, 'think': 1, 'food': 1, 'should': 1, '...   \n",
       "996         {'appetite': 1, 'instantly': 1, 'gone': 1}   \n",
       "997  {'overall': 1, 'i': 1, 'was': 1, 'not': 2, 'im...   \n",
       "998  {'the': 1, 'whole': 1, 'experience': 1, 'was':...   \n",
       "999  {'then': 1, 'as': 1, 'if': 1, 'i': 1, 'hadnt':...   \n",
       "\n",
       "                                          bigram_count  \n",
       "0    {'wow loved': 1, 'loved this': 1, 'this place'...  \n",
       "1          {'crust is': 1, 'is not': 1, 'not good': 1}  \n",
       "2    {'not tasty': 1, 'tasty and': 1, 'and the': 1,...  \n",
       "3    {'stopped by': 1, 'by during': 1, 'during the'...  \n",
       "4    {'the selection': 1, 'selection on': 1, 'on th...  \n",
       "..                                                 ...  \n",
       "995  {'i think': 1, 'think food': 1, 'food should':...  \n",
       "996     {'appetite instantly': 1, 'instantly gone': 1}  \n",
       "997  {'overall i': 1, 'i was': 1, 'was not': 1, 'no...  \n",
       "998  {'the whole': 1, 'whole experience': 1, 'exper...  \n",
       "999  {'then as': 1, 'as if': 1, 'if i': 1, 'i hadnt...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokens_to_token_freq(tokens):\n",
    "    dictionary = dict.fromkeys(tokens)\n",
    "    for i in list(dictionary):\n",
    "        count = 0\n",
    "        for j in tokens:\n",
    "            if j==i:\n",
    "                count += 1\n",
    "        dictionary[i] = count\n",
    "    return dictionary\n",
    "\n",
    "def bigram_list(sent):\n",
    "    bigrams=[]\n",
    "    for i in range(0, len(sent) - 1):\n",
    "        bigrams.append(sent[i] + ' ' + sent[i + 1])\n",
    "    return bigrams\n",
    "\n",
    "df = df.assign(bigrams = df[\"tokens\"].apply(lambda x : dict.fromkeys(bigram_list(x))))\n",
    "df = df.assign(token_count = df[\"tokens\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "df = df.assign(bigram_count = df[\"bigrams\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Print the 10 most frequent bigrams and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('on the', 23), ('the service', 24), ('service was', 26), ('and i', 27), ('of the', 29), ('food was', 32), ('and the', 38), ('it was', 41), ('the food', 47), ('this place', 72)]\n"
     ]
    }
   ],
   "source": [
    "def frequency(column):\n",
    "    frequency = dict(functools.reduce(operator.add, \n",
    "         map(collections.Counter, column)))\n",
    "    frequency = sorted(frequency.items(), key=lambda x: x[1])\n",
    "    return(list(frequency))\n",
    "\n",
    "bigrams = frequency(df[\"bigram_count\"])\n",
    "print(bigrams[len(bigrams) - 10 : ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.) Print a bigram matrix from the 10 most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  4.  0.  0. 10.  0.  0.  0. 29.]\n",
      " [ 0.  0.  0. 10.  2.  1. 41.  2.  3.  0.]\n",
      " [ 0.  0.  0. 20.  1.  0. 10.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  1. 16.  1.  0.  0. 10.]\n",
      " [ 0.  1.  2.  0.  0.  3.  0.  0.  1. 11.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0. 21.  0.  0.  0. 13.]\n",
      " [ 0.  0.  0.  0.  0.  0. 19.  0.  0.  0.]\n",
      " [ 0. 17.  4.  0.  0.  3.  5. 27.  0. 38.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "token_frequency = frequency(df[\"token_count\"])\n",
    "\n",
    "def bigram_matrix(token_list, bigram_frequencies):\n",
    "    matrix = np.zeros((len(token_list), len(token_list)))\n",
    "    for i in range(0, len(matrix)):\n",
    "        for j in range(0, len(matrix)):\n",
    "            key_str = token_list[i][0] + \" \" + token_list[j][0]\n",
    "            bigram_frequencies = dict(bigram_frequencies)\n",
    "            if key_str in bigram_frequencies:\n",
    "                matrix[i, j] = bigram_frequencies[key_str]          \n",
    "    return matrix\n",
    "\n",
    "print(bigram_matrix(token_frequency[len(token_frequency) - 10 : ], bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code of Task 1:\n",
    "\n",
    "a.) Extract top 20 most frequent bigrams of all positive and negative documents in folder polarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('youve got', 4), ('for the', 4), ('jackie chan', 4), ('this film', 5), ('as the', 5), ('and the', 5), ('from the', 5), ('it is', 5), ('the movie', 5), ('the first', 5), ('is a', 6), ('to be', 7), ('is the', 7), ('with a', 7), ('the film', 8), ('with the', 8), ('by the', 8), ('the shark', 9), ('of the', 14), ('in the', 24)]\n",
      "\n",
      "[('most of', 4), ('they are', 4), ('of a', 5), ('and the', 5), ('from the', 5), ('by the', 5), ('is not', 5), ('in a', 6), ('with the', 6), ('for the', 6), ('of course', 6), ('the movie', 6), ('it is', 6), ('film is', 6), ('on the', 7), ('to be', 8), ('that the', 8), ('in the', 8), ('the film', 9), ('of the', 10)]\n"
     ]
    }
   ],
   "source": [
    "pos_docs = os.listdir(\"./polarity/pos\")\n",
    "pos_df = read_file(\"./polarity/pos/\" + pos_docs[0])\n",
    "for doc in pos_docs[1:]:\n",
    "    pos_df = pos_df.append(read_file(\"./polarity/pos/\" + doc))\n",
    "\n",
    "pos_df = pos_df.assign(bigrams = pos_df[\"tokens\"].apply(lambda x : dict.fromkeys(bigram_list(x))))\n",
    "pos_df = pos_df.assign(token_count = pos_df[\"tokens\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "pos_df = pos_df.assign(bigram_count = pos_df[\"bigrams\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "\n",
    "pos_bigrams = frequency(pos_df[\"bigram_count\"])\n",
    "print(pos_bigrams[len(pos_bigrams) - 20:])\n",
    "\n",
    "neg_docs = os.listdir(\"./polarity/neg\")\n",
    "neg_df = read_file(\"./polarity/neg/\" + neg_docs[0])\n",
    "for doc in neg_docs[1:]:\n",
    "    neg_df = neg_df.append(read_file(\"./polarity/neg/\" + doc))\n",
    "\n",
    "print()\n",
    "neg_df = neg_df.assign(neg_bigrams = neg_df[\"tokens\"].apply(lambda x : dict.fromkeys(bigram_list(x))))\n",
    "neg_df = neg_df.assign(token_count = neg_df[\"tokens\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "neg_df = neg_df.assign(bigram_count = neg_df[\"neg_bigrams\"].apply(lambda x : tokens_to_token_freq(x)))\n",
    "# print(neg_df)\n",
    "neg_bigrams = frequency(neg_df[\"bigram_count\"])\n",
    "print(neg_bigrams[len(neg_bigrams) - 20:])\n",
    "# print(neg_df[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.) Calculate their probability as aforementioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Bigram Probabilities\n",
      "[('most of', 0.0016326530612244899), ('they are', 0.0016326530612244899), ('of a', 0.0020408163265306124), ('and the', 0.0020408163265306124), ('from the', 0.0020408163265306124), ('by the', 0.0020408163265306124), ('is not', 0.0020408163265306124), ('in a', 0.0024489795918367346), ('with the', 0.0024489795918367346), ('for the', 0.0024489795918367346), ('of course', 0.0024489795918367346), ('the movie', 0.0024489795918367346), ('it is', 0.0024489795918367346), ('film is', 0.0024489795918367346), ('on the', 0.002857142857142857), ('to be', 0.0032653061224489797), ('that the', 0.0032653061224489797), ('in the', 0.0032653061224489797), ('the film', 0.003673469387755102), ('of the', 0.004081632653061225)]\n",
      "\n",
      "Positive Bigram Probabilities\n",
      "[('youve got', 0.001234186979327368), ('for the', 0.001234186979327368), ('jackie chan', 0.001234186979327368), ('this film', 0.0015427337241592102), ('as the', 0.0015427337241592102), ('and the', 0.0015427337241592102), ('from the', 0.0015427337241592102), ('it is', 0.0015427337241592102), ('the movie', 0.0015427337241592102), ('the first', 0.0015427337241592102), ('is a', 0.0018512804689910522), ('to be', 0.0021598272138228943), ('is the', 0.0021598272138228943), ('with a', 0.0021598272138228943), ('the film', 0.002468373958654736), ('with the', 0.002468373958654736), ('by the', 0.002468373958654736), ('the shark', 0.002776920703486578), ('of the', 0.004319654427645789), ('in the', 0.007405121875964209)]\n"
     ]
    }
   ],
   "source": [
    "top_pos_bigrams = pos_bigrams[len(pos_bigrams) - 20:]\n",
    "top_neg_bigrams = neg_bigrams[len(neg_bigrams) - 20:]\n",
    "\n",
    "def calculate_probabilty(all_bigrams, bigrams):\n",
    "    probability = []\n",
    "    for x in bigrams:\n",
    "        probability.append((x[0], x[1]/sum(dict(all_bigrams).values())))\n",
    "    return probability\n",
    "\n",
    "neg_probability = calculate_probabilty(neg_bigrams, top_neg_bigrams)\n",
    "print(\"Negative Bigram Probabilities\")\n",
    "print(neg_probability)\n",
    "print()\n",
    "\n",
    "pos_probability = calculate_probabilty(pos_bigrams, top_pos_bigrams)\n",
    "print(\"Positive Bigram Probabilities\")\n",
    "print(pos_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.) Write a script to save in a file the most probably bigrams of both classes and their probabilites separated by tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.) Compare the output of both classes and write an analysis of your obersvations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
