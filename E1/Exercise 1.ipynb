{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a program to tokenize the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['While', \"we're\", 'waiting', 'for', \"@LeagueOfLegends's\", '10th', 'anniversary.', \"Let's\", 'have', 'some', 'fun!', 'Tweet', 'me', 'your', 'favorite', 'skin', 'ideas', 'and', \"I'll\", 'send', '1k', 'RP', 'to', 'a', 'couple', 'of', 'my', 'favorites.', 'My', 'friend', 'keeps', 'telling', 'me', \"he'd\", 'like', 'to', 'see', 'ShamWow!', 'Vlad.', '#League10']\n"
     ]
    }
   ],
   "source": [
    "text = 'While we\\'re waiting for @LeagueOfLegends\\'s 10th anniversary. Let\\'s have some fun! Tweet me your favorite skin ideas and I\\'ll send 1k RP to a couple of my favorites. My friend keeps telling me he\\'d like to see ShamWow! Vlad. #League10'\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splits the tokens by whitespace. This method was chosen due to it's simplicity and the fact that it is fairly efficient for most English sentences. The problems that are present are that punctuation such as \"!\" are not considered to be separate tokens. Another question that arises is if thing's such aus \"@LeaugeOfLegend's\" should be considered one token or not. Here it was accepted as one token because of the splitting method chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print a list of all words and their length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While 5\n",
      "we're 5\n",
      "waiting 7\n",
      "for 3\n",
      "@LeagueOfLegends's 18\n",
      "10th 4\n",
      "anniversary. 12\n",
      "Let's 5\n",
      "have 4\n",
      "some 4\n",
      "fun! 4\n",
      "Tweet 5\n",
      "me 2\n",
      "your 4\n",
      "favorite 8\n",
      "skin 4\n",
      "ideas 5\n",
      "and 3\n",
      "I'll 4\n",
      "send 4\n",
      "1k 2\n",
      "RP 2\n",
      "to 2\n",
      "a 1\n",
      "couple 6\n",
      "of 2\n",
      "my 2\n",
      "favorites. 10\n",
      "My 2\n",
      "friend 6\n",
      "keeps 5\n",
      "telling 7\n",
      "me 2\n",
      "he'd 4\n",
      "like 4\n",
      "to 2\n",
      "see 3\n",
      "ShamWow! 8\n",
      "Vlad. 5\n",
      "#League10 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(tokens)):\n",
    "    print(tokens[i], len(tokens[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code simply prints each element of the list tokens createn in part 1 along with the length of the string. The only problem here is that punctuation is added to the length of the word (e.g. \"I'll\" has length of 3, \"favorites.\" a length of 10). This may make sense when dealing with contractions but is most likely suboptimal when dealing with other punctuation. To avoid this a better tokenization method could be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print a list of all words and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while 1\n",
      "we're 1\n",
      "waiting 1\n",
      "for 1\n",
      "@leagueoflegends's 1\n",
      "10th 1\n",
      "anniversary. 1\n",
      "let's 1\n",
      "have 1\n",
      "some 1\n",
      "fun! 1\n",
      "tweet 1\n",
      "me 2\n",
      "your 1\n",
      "favorite 1\n",
      "skin 1\n",
      "ideas 1\n",
      "and 1\n",
      "i'll 1\n",
      "send 1\n",
      "1k 1\n",
      "rp 1\n",
      "to 2\n",
      "a 1\n",
      "couple 1\n",
      "of 1\n",
      "my 2\n",
      "favorites. 1\n",
      "friend 1\n",
      "keeps 1\n",
      "telling 1\n",
      "he'd 1\n",
      "like 1\n",
      "see 1\n",
      "shamwow! 1\n",
      "vlad. 1\n",
      "#league10 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(tokens)):\n",
    "    tokens[i] = tokens[i].lower()\n",
    "#words = tokens without duplicates. Duplicates removed by creating a dictionary from the keys\n",
    "words = list(dict.fromkeys(tokens))\n",
    "for i in range(0, len(words)):\n",
    "    print(words[i], tokens.count(words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first for loop changes all letters to lowercase. This is to ensure that two words that are identical except for the case are still considered one word (e.g. \"my\" and \"My\" will be considered one word and not two). For this particular instance it doesn't make a difference, but for other strings it could. \n",
    "\n",
    "An additional thing to consider is that words with the same lemmas are considered two different words here. For example \"see\" and \"saw\" would be considred two different word even though they come from the same verb. Weather or not this is ideal would need to be decided in a the given context. For the given string it is irrelevant.\n",
    "\n",
    "As punctuation is always in the same token as a word, things like \"favorites\" and \"favorites.\" would be considred two different words even though they are clearly the same. This is suboptimal, but as long as splitting occurs by whitespace (and no further rules) it is unavoidable and for this string also irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [[\"The\", \"changes\", \"were\", \"announced\", \"by\", \"President\", \"Trump\"],\n",
    "         [\"Be\", \"thoughtful\", \"kind\", \"and\", \"mindful\"],\n",
    "         [\"you\", \"will\", \"never\", \"have\", \"this\", \"day\", \"again\"],\n",
    "         [\"Unexpressed\", \"emotions\", \"will\", \"never\", \"die\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these sentences, write a program to:\n",
    "1. calculate the number of words in sentence 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(sents[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. calculate the total count of all words of all four sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "sum_total = 0\n",
    "for x in sents:\n",
    "    sum_total += len(x)\n",
    "print(sum_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words are counted by looping over sents and adding together the length of all the arrays stored in sents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. calculate the total number of letters in all four sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "sum_letters = 0\n",
    "for i in sents:\n",
    "    for j in i:\n",
    "        sum_letters += len(j)\n",
    "print(sum_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letters are counted by looping over sents and then looping over the individual strings in each entry of sents and adding the length of all strings together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. return each sentence as a string, words should be separated by a hyphen and lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the-changes-were-announced-by-president-trump\n",
      "be-thoughtful-kind-and-mindful\n",
      "you-will-never-have-this-day-again\n",
      "unexpressed-emotions-will-never-die\n"
     ]
    }
   ],
   "source": [
    "for i in sents:\n",
    "    sentence = \"\"\n",
    "    for j in i:\n",
    "        sentence += j + \"-\"\n",
    "    print(sentence.lower()[0:len(sentence)-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual strings are created with two for loops. The outer loop loops over the array sents and the second loop over the individual strings in an entry in sents. These are then added together with a \"-\" between every word. At the end, the last character needs to be stripped becasue the for loop adds an additional \"-\" at the end of every string."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
