{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have seen, how to tokenize a document, extract attributes from its tokens and even how to create a bag of words. In our second lab, we used the `CountVectorizer` from `scikit-learn` in order to create a matrix that counts words in a document. Do you remember how that works? \n",
    "\n",
    "The process of vectorizing a text is performed given the fact that most algorithms are not designed to handle raw text. Therefore, we need to represent each text document in a mathematical form, so that calculations can be done. There are several ways for vectorizing text, the easiest one is the bag of words approach, where we create a vocabulary with all the words in our document collection. The idea is to create a matrix to represent which words of my vocabulary are present in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "Use the text provided below to create a bag of words using the `CountVectorizer` function as we did in a previous session. \n",
    "\n",
    "**Steps** \n",
    "Print your vocabulary and print the array form of your vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['Elon Musk wants to build a Gigafactory',\n",
    "        'UK is too risky after the Brexit for a Gigafactory',\n",
    "        'Tesla wants to build a Gigafactory in Berlin',\n",
    "        'Brexit has made it too risky for Tesla to put a Gigafactory in the UK.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'berlin', 'brexit', 'build', 'elon', 'for', 'gigafactory', 'has', 'in', 'is', 'it', 'made', 'musk', 'put', 'risky', 'tesla', 'the', 'to', 'too', 'uk', 'wants']\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(docs)\n",
    "print(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1]\n",
      " [1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1]\n",
      " [0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we also know that counting is not the only way of representing text with numbers. We can also penalize tokens that occurr very often. Why?\n",
    "\n",
    "Because in general, very frequent token sometimes are not as relevant as tokens that appear less often. For instance, if we want to retrieve documents that are relevant for a query: _The President Donald Trump_, one idea would be to retrieve all document containing all the words in the query. However, since our search retrieved still many documents, we might want to count the times that query words appear in the selected documents. But, _the_ and _president_, may still have many occurrences. Therefore, we should focus on documents that contain rather _donald trump_. But how do we get there? Calculating term frequencyâ€“inverse document frequency (tf-idf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tf-Idf**: term frequency of a token, multiplied by the inverse document frequency (log[number of documents containing a token]).\n",
    "\n",
    "- Tf: Term frequency: $\\frac{freq(term)}{\\# terms \\in doc} $\n",
    "- Idf: Inverse document frequency: $\\log\\frac{|D|}{\\# d : term \\in doc}$\n",
    "\n",
    "Notice that we calculate the tf-idf for each term in each document. Let's calculate them for _Elon_ and _Gigafactory_ in the first document.\n",
    "\n",
    "**Examples:**\n",
    "- Tf-Idf(Elon) = $\\frac{1}{7}*\\log(\\frac{4}{1}) = 0.14*0.6  = 0.084$\n",
    "- Tf-Idf(Gigafactory) = $\\frac{1}{7}*\\log(\\frac{4}{4}) = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "Use the same data and the `TfidfVectorizer`, create a matrix and explore several attributes as input for the vectorizer. \n",
    "\n",
    "**Steps** \n",
    "Use the `?` to explore the function of the vectorizer and refer to [this link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) for more options about the vectorizer. Increase the range of ngrams and observe the matrix. Can you see any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method CountVectorizer.get_feature_names of TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)>\n",
      "[[0.         0.         0.         0.39806    0.50488863 0.\n",
      "  0.26347183 0.         0.         0.         0.         0.\n",
      "  0.50488863 0.         0.         0.         0.         0.32226387\n",
      "  0.         0.         0.39806   ]\n",
      " [0.40818453 0.         0.32181737 0.         0.         0.32181737\n",
      "  0.21300762 0.         0.         0.40818453 0.         0.\n",
      "  0.         0.         0.32181737 0.         0.32181737 0.\n",
      "  0.32181737 0.32181737 0.        ]\n",
      " [0.         0.48993129 0.         0.38626746 0.         0.\n",
      "  0.25566647 0.         0.38626746 0.         0.         0.\n",
      "  0.         0.         0.         0.38626746 0.         0.3127168\n",
      "  0.         0.         0.38626746]\n",
      " [0.         0.         0.25376616 0.         0.         0.25376616\n",
      "  0.16796522 0.3218702  0.25376616 0.         0.3218702  0.3218702\n",
      "  0.         0.3218702  0.25376616 0.25376616 0.25376616 0.20544558\n",
      "  0.25376616 0.25376616 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1),)\n",
    "train_tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names)\n",
    "print(train_tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore these vectorizers with real data. You probably remember our data of reviews on Yelp. Let's use it to compare both vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"yelp_polarity.txt\", sep=\"\\t\", header=None)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "In order to train any model, you need to split your data. The reason for this is that you want to test the performance of your classifier at the end. And this can't be done on the same data you train. Therefore, you need to keep a small set of data that you never use until you test. Let's create our train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, label_train, label_test = train_test_split(data[0], data[1], \n",
    "                                                                  test_size=0.20, \n",
    "                                                                  random_state=1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "Use only the train set to generate again a feature matrix using the `CountVectorizer`. This one will be used to train our classifier\n",
    "\n",
    "**Hint:** Please notice that you need to instantiate again a new vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_count_vectorizer = CountVectorizer()\n",
    "polarity_bow_matrix = polarity_count_vectorizer.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay!!! Finally we're ready to train our first model. In order to do so, we need to input the train features and their labels. In this case we will use a support vector machine (svm). If you haven't heard anything about SVMs yet, don't worry, this won't be difficult, we will use the one provided by sklearn so you don't have to implement it by yourself. They treat each documents as a vector. \n",
    "\n",
    "Imagine all your documents are sample point in a scatter plot. The job of SVMs is to draw a line (hyperplane) in the middle of two classes so that the hyperplane is the widest gap between both of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.fit(polarity_bow_matrix, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after training we can test... But first, we need to convert our test data into numerical features. Now is your turn to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "Vectorize your test set, in the same way we did with the train data. After vectorizing use the method `predict()` and put your test_matrix in the parentesis. This method returns a class prediction for each document in the matrix. \n",
    "\n",
    "Use NumPy to compare the original labels with the ones predicted by our model.\n",
    "\n",
    "**Hint:** Check np.sum and np.equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf_matrix = polarity_count_vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = classifier.predict(test_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers = np.sum(np.equal(test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = correct_answers / (len(test)*1.0) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word vectors from spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy offers pretrained vectors for each token. They come inside the models that we load. However, the small model doesn't include vectors. Let's look at what happen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"We didn't want to eat.\"\n",
    "\n",
    "tokens = nlp(raw)\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov) # oov = Out of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "Download `en_core_web_md`, which is the medium model for English and try to test the token similarity again.\n",
    "\n",
    "**Hint**\n",
    "Call your object different from nlp, so that you can compare without overwriting the nlp object (maybe nlp_medium is a good idea :)). We call these tokens `new_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md\n",
    "# nlp_medium = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = nlp_medium(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token1 in new_tokens:\n",
    "    for token2 in new_tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_tokens[0].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tada!!! And here we finish again another lab session. This time meeting some word vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
