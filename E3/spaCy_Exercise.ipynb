{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "\n",
    "Is an open-source library for NLP in Python, which supports a wide variety of languages. One big advantage of using spaCy is that it's desined to be integrated in  real products without big difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "In order to start using spaCy you need to specify which language class you are going to use. Remember that spaCy was created to be used for several languages, therefore it doesn't asume that you want to use English, you need to explicit specify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note\n",
    "# If you haven't intalled spaCy, please uncomment next line and run this cell\n",
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with an example in English. Since we already know how to tokenize a text, let's take a look of how spaCy does this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import English\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "raw = \"Hard to judge whether these sides were good. We were grossed \" \\\n",
    "      \"out by the melted styrofoam and didn't want to eat it for fear of getting sick.\"\n",
    "\n",
    "doc = nlp(raw)\n",
    "\n",
    "print(doc)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's your turn to do the same for the following Spanish text taken from BBC in Spanish.\n",
    "spanish_raw = '¿Es posible \"desconectar\" a un país entero de internet? ' \\\n",
    "              'La respuesta corta es \"sí\".'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "spaCy uses the same syntax as Python for indexing. This way you can address specific tokens in your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_word = doc[-1]\n",
    "first_word = doc[0]\n",
    "print(first_word, last_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every token in our document has some characteristics that are know in spaCy as **lexical attributes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_word.is_digit)\n",
    "print(last_word.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do we need indexing for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents and spans\n",
    "\n",
    "A token or a sequence of them can be referred as a span. In some NLP tasks spans are very relevant. For instance, in areas as Question Answering (QA), obtaining the correct span that answers a query is a crucial for the task itself. with spaCy, we can also define spans and use their lexical attributes in the same way as we can do it for a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = doc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for you to explore more about lexical attributes on the previous text. \n",
    "# Check this link: https://spacy.io/api/token for more attributes.\n",
    "# What can you comment about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get a bit deeper in statistics\n",
    "\n",
    "In our last exercise we could play around with probabilities. Working with language requires most of the time statistics to solve problems. As an example, we can decide if a the word _tweet_ refers to a noun or to a verb by counting. Can you tell why?\n",
    "\n",
    "Knowing the context of a word and counting how often our desired word appears after a verb or after a noun would give us the probability that we are searching for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we include statistics in spaCy?\n",
    "\n",
    "The good news is that spaCy provides pre-trained models that we can use depending on our necessities. There is an offer of small, medium and large models for different languages. Having such a model, we can use attributes in context. But what exactly is contained in a pre-trained model? It contains a vocabulary of the words used to train our model, their weights and meta-information useful for spaCy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and use a small model for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment next line and run this cell only if you haven't done it before.\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the model is as simple as telling spaCy the name of the model to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we already know what to do..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's your turn to create a new document of our English text \n",
    "# and define a span for its last two words excluding the dot.\n",
    "\n",
    "# new_doc =\n",
    "# last_span = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now display part-of-speech tags, dependencies and lemma for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure inside spaCy\n",
    "\n",
    "Until this point, we have seen how to pass raw text to spaCy and process it into lexical features. However, keeping every token for every occurrence in a text is memory expensive. Therefore, spaCy manages everything in a sort of `internal structure`. \n",
    "\n",
    "This structure has three levels or components, the document (doc), a vocabulary called **vocab** and a lookup table that is called in spaCy the **string store**. The vocab contains token ids also known as **hashes**. From now on, we will call every entry in vocab a **lexeme**. A look-up table indicates which token correponds to which lexeme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "- A document contains tokens with their lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in last_span:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each object in our vocab is a lexeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = nlp.vocab[last_span[1].text]\n",
    "print(lexeme.text, lexeme.orth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each string representation of a hash id can be search in the string store and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_string = nlp.vocab.strings[lexeme.orth]\n",
    "searched_hash = nlp.vocab.strings[lexeme.text]\n",
    "\n",
    "print(\"This is my desired string:\", searched_string)\n",
    "print(\"This is my desired hash:\", searched_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for specific patterns with Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy provides a `Matcher`, which works similar to regular expressions in Python. The difference is that you can search not only the text, but also other token attributes. In this way we could for example differentiate between _tweet_ being a verb or a noun and search only for noun appearances.\n",
    "\n",
    "Here, we have examples of searching text, lexical attributes for a specific token and lexical attributes in a more general search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Google Inc. is a company that has a big development in NLP. \" \\\n",
    "          \"When users google for a word or any query, their system internally \" \\\n",
    "          \"runs a pipeline in order to process what the person is querying.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching exact text\n",
    "\n",
    "pattern_text = [{'TEXT': 'Google'}, {'TEXT': 'Inc.'}]\n",
    "\n",
    "# Match lexical attributes\n",
    "\n",
    "pattern_attr = [{'LOWER': 'google'}]\n",
    "\n",
    "# Match any token attributes with these characteristics\n",
    "\n",
    "pattern_gen_attr = [{'LEMMA': 'query'}, {'IS_PUNCT': True}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('PATTERN_TEXT', None, pattern_text)\n",
    "matcher.add('PATTERN_ATTR', None, pattern_attr)\n",
    "matcher.add('PATTERN_GEN_ATTR', None, pattern_gen_attr)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(example)\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matches)\n",
    "print(\"Total of matches found:\", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what can we do with this output? What does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Matcher` returns a list of tuples indicating start and end of each found matched span. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a list of found matches\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for you to suggest a piece of text and create patterns. \n",
    "# The main idea for those patterns is to disambiguate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following what we have seen until now, download a Spanish and a German model and create patterns to find several tokens with more than one ocurrence in the text given in following cells. \n",
    "\n",
    "#### Hint!\n",
    "Notice that models for languages other than English were trained on news data instead of web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download models\n",
    "# load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.examples import sentences \n",
    "raw_spanish = sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.examples import sentences\n",
    "raw_german = sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for you to create your patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
