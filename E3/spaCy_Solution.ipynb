{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "\n",
    "Is an open-source library for NLP in Python, which supports a wide variety of languages. One big advantage of using spaCy is that it's desined to be integrated in  real products without big difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "In order to start using spaCy you need to specify which language class you are going to use. Remember that spaCy was created to be used for several languages, therefore it doesn't asume that you want to use English, you need to explicit specify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note\n",
    "# If you haven't intalled spaCy, please uncomment next line and run this cell\n",
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with an example in English. Since we already know how to tokenize a text, let's take a look of how spaCy does this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard to judge whether these sides were good. We were grossed out by the melted styrofoam and didn't want to eat it for fear of getting sick.\n",
      "Hard\n",
      "to\n",
      "judge\n",
      "whether\n",
      "these\n",
      "sides\n",
      "were\n",
      "good\n",
      ".\n",
      "We\n",
      "were\n",
      "grossed\n",
      "out\n",
      "by\n",
      "the\n",
      "melted\n",
      "styrofoam\n",
      "and\n",
      "did\n",
      "n't\n",
      "want\n",
      "to\n",
      "eat\n",
      "it\n",
      "for\n",
      "fear\n",
      "of\n",
      "getting\n",
      "sick\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Import English\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "raw = \"Hard to judge whether these sides were good. We were grossed \" \\\n",
    "      \"out by the melted styrofoam and didn't want to eat it for fear of getting sick.\"\n",
    "\n",
    "doc = nlp(raw)\n",
    "\n",
    "print(doc)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Es posible \"desconectar\" a un país entero de internet? La respuesta corta es \"sí\".\n",
      "¿\n",
      "Es\n",
      "posible\n",
      "\"\n",
      "desconectar\n",
      "\"\n",
      "a\n",
      "un\n",
      "país\n",
      "entero\n",
      "de\n",
      "internet\n",
      "?\n",
      "La\n",
      "respuesta\n",
      "corta\n",
      "es\n",
      "\"\n",
      "sí\n",
      "\"\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Now it's your turn to do the same for the following Spanish text taken from BBC in Spanish.\n",
    "\n",
    "spanish_raw = '¿Es posible \"desconectar\" a un país entero de internet? ' \\\n",
    "              'La respuesta corta es \"sí\".'\n",
    "\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "es_nlp = Spanish()\n",
    "\n",
    "es_doc = es_nlp(spanish_raw)\n",
    "\n",
    "print(es_doc)\n",
    "\n",
    "for token in es_doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "spaCy uses the same syntax as Python for indexing. This way you can address specific tokens in your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard .\n"
     ]
    }
   ],
   "source": [
    "last_word = doc[-1]\n",
    "first_word = doc[0]\n",
    "print(first_word, last_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every token in our document has some characteristics that are know in spaCy as **lexical attributes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      ".\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(first_word.is_digit)\n",
    "print(last_word)\n",
    "print(last_word.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do we need indexing for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents and spans\n",
    "\n",
    "A token or a sequence of them can be referred as a span. In some NLP tasks spans are very relevant. For instance, in areas as Question Answering (QA), obtaining the correct span that answers a query is a crucial for the task itself. with spaCy, we can also define spans and use their lexical attributes in the same way as we can do it for a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = doc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these sides were good.\n"
     ]
    }
   ],
   "source": [
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a part-of-speech tag: \n"
     ]
    }
   ],
   "source": [
    "# This cell is reserved for you to explore more about lexical attributes on the previous text. \n",
    "# Check this link: https://spacy.io/api/token for more attributes.\n",
    "# What can you comment about?\n",
    "print(\"Here is a part-of-speech tag:\", last_word.pos_) # Why is it empty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get a bit deeper in statistics\n",
    "\n",
    "In our last exercise we could play around with probabilities. Working with language requires most of the time statistics to solve problems. As an example, we can decide if a the word _tweet_ refers to a noun or to a verb by counting. Can you tell why?\n",
    "\n",
    "Knowing the context of a word and counting how often our desired word appears after a verb or after a noun would give us the probability that we are searching for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we include statistics in spaCy?\n",
    "\n",
    "The good news is that spaCy provides pre-trained models that we can use depending on our necessities. There is an offer of small, medium and large models for different languages. Having such a model, we can use attributes in context. But what exactly is contained in a pre-trained model? It contains a vocabulary of the words used to train our model, their weights and meta-information useful for spaCy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and use a small model for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz#egg=en_core_web_sm==2.2.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0MB 377kB/s eta 0:00:01     |████████████████████████████▎   | 10.6MB 358kB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.0 in /usr/lib/python3.7/site-packages (from en_core_web_sm==2.2.0) (2.2.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.17.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.23)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (7.3.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (41.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (0.6.0)\n",
      "Requirement already satisfied: chardet>=3.0.2 in /usr/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (1.25.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.28.1)\n",
      "Requirement already satisfied: more_itertools in /usr/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (7.2.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /usr/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ukllw283/en-core-web-sm/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ukllw283/en-core-web-sm/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-u9qqfikz/install-record.txt --single-version-externally-managed --compile\n",
      "         cwd: /tmp/pip-install-ukllw283/en-core-web-sm/\n",
      "    Complete output (33 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib\n",
      "    creating build/lib/en_core_web_sm\n",
      "    copying en_core_web_sm/__init__.py -> build/lib/en_core_web_sm\n",
      "    creating build/lib/en_core_web_sm/en_core_web_sm-2.2.0\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/meta.json -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/accuracy.json -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/tokenizer -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0\n",
      "    creating build/lib/en_core_web_sm/en_core_web_sm-2.2.0/parser\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/parser/moves -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/parser\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/parser/model -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/parser\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/parser/cfg -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/parser\n",
      "    creating build/lib/en_core_web_sm/en_core_web_sm-2.2.0/vocab\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/vocab/vectors -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/vocab\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/vocab/key2row -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/vocab\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/vocab/lookups.bin -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/vocab\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/vocab/strings.json -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/vocab\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/vocab/lexemes.bin -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/vocab\n",
      "    creating build/lib/en_core_web_sm/en_core_web_sm-2.2.0/ner\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/ner/moves -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/ner\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/ner/model -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/ner\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/ner/cfg -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/ner\n",
      "    creating build/lib/en_core_web_sm/en_core_web_sm-2.2.0/tagger\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/tagger/model -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/tagger\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/tagger/cfg -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/tagger\n",
      "    copying en_core_web_sm/en_core_web_sm-2.2.0/tagger/tag_map -> build/lib/en_core_web_sm/en_core_web_sm-2.2.0/tagger\n",
      "    copying en_core_web_sm/meta.json -> build/lib/en_core_web_sm\n",
      "    running install_lib\n",
      "    creating /usr/lib/python3.7/site-packages/en_core_web_sm\n",
      "    error: could not create '/usr/lib/python3.7/site-packages/en_core_web_sm': Permission denied\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ukllw283/en-core-web-sm/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ukllw283/en-core-web-sm/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-u9qqfikz/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Uncomment next line and run this cell only if you haven't done it before.\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the model is as simple as telling spaCy the name of the model to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-12102ff9e1a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we already know what to do..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n"
     ]
    }
   ],
   "source": [
    "# It's your turn to create a new document of our English text \n",
    "# and define a span for its last two words excluding the dot.\n",
    "\n",
    "# new_doc =\n",
    "# last_span = \n",
    "new_doc = nlp(raw)\n",
    "\n",
    "word_two = new_doc[1]\n",
    "last_span = new_doc[-3:-1]\n",
    "print(word_two.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting   getting\n",
      "sick   sick\n"
     ]
    }
   ],
   "source": [
    "# Now display part-of-speech tags, dependencies and lemma for them.\n",
    "for token in last_span:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure inside spaCy\n",
    "\n",
    "Until this point, we have seen how to pass raw text to spaCy and process it into lexical features. However, keeping every token for every occurrence in a text is memory expensive. Therefore, spaCy manages everything in a sort of `internal structure`. \n",
    "\n",
    "This structure has three levels or components, the document (doc), a vocabulary called **vocab** and a lookup table that is called in spaCy the **string store**. The vocab contains token ids also known as **hashes**. From now on, we will call every entry in vocab a **lexeme**. A look-up table indicates which token correponds to which lexeme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "- A document contains tokens with their lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in last_span:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each object in our vocab is a lexeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = nlp.vocab[last_span[1].text]\n",
    "print(lexeme.text, lexeme.orth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each string representation of a hash id can be search in the string store and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_string = nlp.vocab.strings[lexeme.orth]\n",
    "searched_hash = nlp.vocab.strings[lexeme.text]\n",
    "\n",
    "print(\"This is my desired string:\", searched_string)\n",
    "print(\"This is my desired hash:\", searched_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for specific patterns with Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy provides a `Matcher`, which works similar to regular expressions in Python. The difference is that you can search not only the text, but also other token attributes. In this way we could for example differentiate between _tweet_ being a verb or a noun and search only for noun appearances.\n",
    "\n",
    "Here, we have examples of searching text, lexical attributes for a specific token and lexical attributes in a more general search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Google Inc. is a company that has a big development in NLP. \" \\\n",
    "          \"When users google for a word or any query, their system internally \" \\\n",
    "          \"runs a pipeline in order to process what the person is querying.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching exact text\n",
    "\n",
    "pattern_text = [{'TEXT': 'Google'}, {'TEXT': 'Inc.'}]\n",
    "\n",
    "# Match lexical attributes\n",
    "\n",
    "pattern_attr = [{'LOWER': 'google'}]\n",
    "\n",
    "# Match any token attributes with these characteristics\n",
    "\n",
    "pattern_gen_attr = [{'LEMMA': 'query'}, {'IS_PUNCT': True}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('PATTERN_TEXT', None, pattern_text)\n",
    "matcher.add('PATTERN_ATTR', None, pattern_attr)\n",
    "matcher.add('PATTERN_GEN_ATTR', None, pattern_gen_attr)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(example)\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matches)\n",
    "print(\"Total of matches found:\", len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what can we do with this output? What does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Matcher` returns a list of tuples indicating start and end of each found matched span. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a list of found matches\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for you to suggest a piece of text and create patterns. \n",
    "# The main idea for those patterns is to disambiguate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following what we have seen until now, download a Spanish and a German model and create patterns to find several tokens with more than one ocurrence in the text given in following cells. \n",
    "\n",
    "#### Hint!\n",
    "Notice that models for languages other than English were trained on news data instead of web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_nlp = spacy.load('es_core_news_sm')\n",
    "de_nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.examples import sentences \n",
    "raw_spanish = sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.examples import sentences\n",
    "raw_german = sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for you to create your patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
